import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Loading and preprocessing the dataset
def load_and_preprocess_data(file_path):
    df = pd.read_csv(file_path)
    print("Class Distribution (is_attack):")
    print(df['is_attack'].value_counts(normalize=True))
    features = ['time_delta', 'pps', 'length', 'src_ip_entropy', 'syn_flag', 'ack_flag']
    X = df[features]
    y = df['is_attack']
    X = X.fillna(X.mean())
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled, y, features, scaler

# Training base models and generating meta-features
def train_base_models(X_train, y_train, X_test):
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)
    rf_model.fit(X_train, y_train)
    xgb_model.fit(X_train, y_train)
    rf_pred_train = rf_model.predict_proba(X_train)[:, 1]
    xgb_pred_train = xgb_model.predict_proba(X_train)[:, 1]
    rf_pred_test = rf_model.predict_proba(X_test)[:, 1]
    xgb_pred_test = xgb_model.predict_proba(X_test)[:, 1]
    meta_features_train = np.column_stack((rf_pred_train, xgb_pred_train))
    meta_features_test = np.column_stack((rf_pred_test, xgb_pred_test))
    return meta_features_train, meta_features_test, rf_model, xgb_model

# Training the stacking ensemble
def train_stacking_ensemble(meta_features_train, y_train, meta_features_test):
    meta_learner = LogisticRegression(random_state=42)
    meta_learner.fit(meta_features_train, y_train)
    final_predictions = meta_learner.predict(meta_features_test)
    return final_predictions, meta_learner

# Evaluating the model
def evaluate_model(y_test, predictions):
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, zero_division=0)
    recall = recall_score(y_test, predictions, zero_division=0)
    f1 = f1_score(y_test, predictions, zero_division=0)
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.savefig('confusion_matrix.png')
    plt.close()
    return cm

# Plotting feature importance for Random Forest
def plot_feature_importance(rf_model, features):
    importances = rf_model.feature_importances_
    indices = np.argsort(importances)[::-1]
    plt.figure(figsize=(8, 6))
    plt.title('Feature Importance (Random Forest)')
    plt.bar(range(len(features)), importances[indices], align='center')
    plt.xticks(range(len(features)), [features[i] for i in indices], rotation=45)
    plt.xlabel('Features')
    plt.ylabel('Importance')
    plt.tight_layout()
    plt.savefig('feature_importance.png')
    plt.close()

# Main function
def main():
    file_path = r"C:\Users\DELL\Desktop\Final_year_Project\Datas\DDoS\Correct\One-attack, labeled.csv"
    X, y, features, scaler = load_and_preprocess_data(file_path)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    meta_features_train, meta_features_test, rf_model, xgb_model = train_base_models(X_train, y_train, X_test)
    final_predictions, meta_learner = train_stacking_ensemble(meta_features_train, y_train, meta_features_test)
    print("Stacking Ensemble Performance:")
    cm = evaluate_model(y_test, final_predictions)
    print("\nConfusion Matrix Details:")
    print(f"True Negatives (Non-Attack, Predicted Non-Attack): {cm[0,0]}")
    print(f"False Positives (Non-Attack, Predicted Attack): {cm[0,1]}")
    print(f"False Negatives (Attack, Predicted Non-Attack): {cm[1,0]}")
    print(f"True Positives (Attack, Predicted Attack): {cm[1,1]}")
    plot_feature_importance(rf_model, features)
    # Save model components
    joblib.dump(meta_learner, 'stacking_model.pkl')
    joblib.dump(rf_model, 'rf_model.pkl')
    joblib.dump(xgb_model, 'xgb_model.pkl')
    joblib.dump(scaler, 'scaler.pkl')
    print("Model components saved as .pkl files.")

main()
